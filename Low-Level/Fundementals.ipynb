{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad39f6b4",
   "metadata": {},
   "source": [
    "### Aside Kuda Kernels\n",
    "\n",
    "- *Question to AI: What is a streaming multiprocessor, explain using relevant terms. Threads, Blocks, Grid, Warps*\n",
    "- **SM (Streaming Multiprocessor)**\n",
    "    - The core execution unit inside an NVIDIA GPU\n",
    "    - Each GPU contains many **SMs**\n",
    "    - Each SM contains:\n",
    "        - **CUDA cores** (scalar/vector ALUs for general math)\n",
    "        - **Tensor Cores** (matrix-math accelerators)\n",
    "        - **Special Function Units** (SFUs — trig, log, etc)\n",
    "        - **Registers**\n",
    "        - **Shared Memory / L1 Cache**\n",
    "        - **Warp Schedulers** (assign work to execution units)\n",
    "- When you launch a CUDA kernel, you don’t talk to SMs directly, you define **threads, blocks and grids**, and the hardware scheduler maps them to SMs\n",
    "    - **Thread:** Smallest execution unit — executes the kernel code\n",
    "    - **Block**: A 1D, 2D, or 3D group of threads. All threads in a block share **shared memory** and can sync\n",
    "    - **Grid:** A collection of thread **blocks**\n",
    "    - **Warp:** A group of **32 threads** that execute in lockstep (SIMT — Single Instruction, Multiple Threads)\n",
    "        - The warp scheduler on each SM sends the same instruction to all threads in the warp, but each threads works on its own data\n",
    "- How SMs Fit into This\n",
    "    - When you launch a kernel:\n",
    "        1. The grid is split into **thread blocks**\n",
    "        2. The GPU assigns each block to an available **SM**\n",
    "        3. Inside an SM:\n",
    "            1. Each block’s threads are split into **warps** (32 threads each)\n",
    "            2. The **warp scheduler** picks which warp runs next, hiding memory latency by swapping to a ready warp\n",
    "            3. Execution units in the SM — CUDA cores, Tensor Cores, SFUs — do the work\n",
    "- SM Example with Tensor Cores\n",
    "    \n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b4b79",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "kernel<<< gridDim=4, blockDim=256 >>>(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972eecc",
   "metadata": {},
   "source": [
    "- Total threads: 4 * 256 = 1024 threads\n",
    "    - Each block (256 threads) might be scheduled on a different SM\n",
    "    - Inside each SM:\n",
    "        - 256 threads = 8 warps (256 / 32)\n",
    "        - The warp scheduler assigns these warps to execution units\n",
    "        - If a warp executes a matrix-multiply instruction, the SM's **Tensor Cores** process it\n",
    "        - If a warp executes simple add/mul, the **CUDA** cores handle it.\n",
    "- Why SM Design Matters\n",
    "    - SMs **hide latency** by switching between warps instantly\n",
    "    - More SMs = more warps in flight = higher throughput\n",
    "    - Tensor Cores inside SMs massively accelerate matrix operations, but they only work when the warp issues special MMA (Matrix Multiplication Accumulator) instructions — otherwise, CUDA cores do the math.\n",
    "\n",
    "```\n",
    "GPU\n",
    "├── SM 0\n",
    "│   ├── Warp Scheduler\n",
    "│   ├── CUDA Cores (scalar ops)\n",
    "│   ├── Tensor Cores (matrix ops)\n",
    "│   ├── Shared Memory\n",
    "│   └── Registers\n",
    "├── SM 1\n",
    "│   └── ...\n",
    "└── SM N\n",
    "\n",
    "Thread Hierarchy:\n",
    "Grid\n",
    "└── Block (shared mem) → assigned to SM\n",
    "    └── Warp (32 threads)\n",
    "        └── Thread (1 execution context)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
