{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c36659c",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## Memory Wall\n",
    "\n",
    "- Every few years the number of transistors on a microchip increases, and as a result the capacity for performing arithmetic operations has increased exponentially.\n",
    "\n",
    "    - A **transistor** is a tiny electronic switch — the basic “work unit” of digital logic and memory. Modern CPUs and GPUs have billions of them.\n",
    "\n",
    "    - More transistors enable more logic gates, memory cells, and specialized units → greater parallelism, higher throughput, and more complex computations.\n",
    "\n",
    "- However, data movement capacity (memory bandwidth) hasn’t increased as fast, creating the **memory wall** — a key bottleneck in deep learning and Tensor Core workloads.\n",
    "\n",
    "- Hence if we want to fully utilize Tensor Cores, we must increase the number of bytes moved between DRAM and compute units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4b4c9",
   "metadata": {},
   "source": [
    "## Roofline Charts \n",
    "\n",
    "- The roofline model states, that performance will be limited by **one of two things**\n",
    "    - **Compute:**\n",
    "        - Data is readily available in fast memory\n",
    "        - The bottleneck is the number of floating-point operations per second (**FLOP/s)** your hardware can execute\n",
    "        - Adding more **bandwidth** here won’t help — only faster **ALUs (Arithmetic Logical Unit: Used to perform computations)**, **Tensor Cores**, or more **parallelism** will improve performance\n",
    "    - **Memory-bound:**\n",
    "        - The **compute units can’t be** **fed fast enough** because data has to be fetched from slow memory (**DRAM**)\n",
    "        - The bottleneck is **memory bandwidth (β bytes/sec)**\n",
    "        - Doubling **compute** units won’t help unless you also increase your **bandwidth** or improve data reuse in fast memory.\n",
    "    - Plot Interpretation:\n",
    "        - **X-axis:** Operation intensity = **FLOPs** per byte transferred (how much work you do per data fetched)\n",
    "        - **Y-axis:** Achievable performance (**FLOP/s)**\n",
    "        - The **roof** has two segments:\n",
    "            - **Slopped line →** memory-bound region\n",
    "            - **Flat line** → compute-bound region\n",
    "            - Where you land depends on how much computation you can do before you have to fetch more data\n",
    "- **Two-level memory model in GPUs:**\n",
    "    - **Fast memory** (shared memory in the SM) is physically close to the compute units.\n",
    "    - **Slow memory** (DRAM) is farther away, so accessing it takes longer.\n",
    "    - Computation can only be performed at peak rate (**τ FLOP/s**) on data already in fast memory.\n",
    "    - Slow memory can transfer data into fast memory at **β bytes/sec**\n",
    "    - Because of the memory wall, we want to move as much data as possible, and as fast as possible.\n",
    "        - Hence, since **τ** is larger than **β,** we want to maximize time spent operating in in **fast memory,** and minimize **slow memory accesses**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9614c835",
   "metadata": {},
   "source": [
    "![image.png](images\\GEMM2\\roof_line.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
