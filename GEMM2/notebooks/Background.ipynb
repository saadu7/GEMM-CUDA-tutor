{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c36659c",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## Memory Wall\n",
    "\n",
    "- Every few years the number of transistors on a microchip increases, and as a result the capacity for performing arithmetic operations has increased exponentially.\n",
    "\n",
    "    - A **transistor** is a tiny electronic switch — the basic “work unit” of digital logic and memory. Modern CPUs and GPUs have billions of them.\n",
    "\n",
    "    - More transistors enable more logic gates, memory cells, and specialized units → greater parallelism, higher throughput, and more complex computations.\n",
    "\n",
    "- However, data movement capacity **(memory bandwidth)** hasn’t increased as fast, creating the **memory wall** — a key bottleneck in deep learning and Tensor Core workloads.\n",
    "    - Bandwidth is how much **data per second** your memory system can deliver to (and accept from) the **compute units**\n",
    "    - **Analogy**:\n",
    "        - Think of a highawy:\n",
    "            - Cars = **bytes**\n",
    "            - Number of lanes = **bus width**\n",
    "            - Speed limit = **clock rate**\n",
    "            - Toll booths/merges = **memory controllers/channels**\n",
    "        - How many cars reach the city each second? That's the bandwidth\n",
    "    - Do not confuse **bandwidth** with **latency**:\n",
    "        - **Latency** is how long one trip takes\n",
    "        - **Bandwidth** is how many bytes per second you can keep flowing\n",
    "\n",
    "- Hence if we want to fully utilize Tensor Cores, we must increase the number of bytes moved between DRAM and compute units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374847f",
   "metadata": {},
   "source": [
    "![image.png](../../images/GEMM2/simple_computer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4b4c9",
   "metadata": {},
   "source": [
    "## Roofline Charts \n",
    "\n",
    "- The roofline model states, that performance will be limited by **one of two things**\n",
    "    - **Compute:**\n",
    "        - Data is readily available in fast memory\n",
    "        - The bottleneck is the number of floating-point operations per second (**FLOP/s)** your hardware can execute\n",
    "        - Adding more **bandwidth** here won’t help — only faster **ALUs (Arithmetic Logical Unit: Used to perform computations)**, **Tensor Cores**, or more **parallelism** will improve performance\n",
    "    - **Memory-bound:**\n",
    "        - The **compute units can’t be** **fed fast enough** because data has to be fetched from slow memory (**DRAM**)\n",
    "        - The bottleneck is **memory bandwidth (β bytes/sec)**\n",
    "        - Doubling **compute** units won’t help unless you also increase your **bandwidth** or improve data reuse in fast memory.\n",
    "    - Plot Interpretation:\n",
    "        - **X-axis:** Operation intensity = **FLOPs** per byte transferred (how much work you do per data fetched)\n",
    "        - **Y-axis:** Achievable performance (**FLOP/s)**\n",
    "        - The **roof** has two segments:\n",
    "            - **Slopped line →** memory-bound region\n",
    "            - **Flat line** → compute-bound region\n",
    "            - Where you land depends on how much computation you can do before you have to fetch more data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9614c835",
   "metadata": {},
   "source": [
    "![image.png](../../images/GEMM2/roof_line.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c57ff",
   "metadata": {},
   "source": [
    "- **Explanation**\n",
    "    - Any given computation has a certain number of FLOPs that need to be performed. For example, if you want to multiply a M by K matrix with a K by N matrix we need to perform $2 * M * N * K$ FLOPs.\n",
    "        - For each pair $(i, j)$ you do K multiplies, and K - 1 adds $\\approx{2K FLOPS}$. 1 FLOP(Floating point operation) for multiply + 1 FLOP for addition\n",
    "        - There are M * N outputs, so total $FLOPs \\approx{(M N) * (2K)} ={2*M*N*K}$\n",
    "    - The more FLOPs/sec our algorithm can achieve, the faster we can get the matrix multiplication done.\n",
    "    - The roofline model gives us an upper bound on the FLOPs/sec we can achieve, subject to $\\tau$ and $\\beta$ which are fixed properties of our hardware.\n",
    "        - $\\tau$ (tau) = the **peak compute throughput** of your device for a given datatype/op (e.g **FP32** (Floating Point 32-bit float), **FP16/Tensor**)\n",
    "            - $\\tau$ is typically a large number. For example, for the T4 GPU, $\\tau$= 65,000,000,000,000 FLOPs/second. Units: FLOP/s\n",
    "        - $\\beta$ (beta) = the **peak sustained memory bandwidth** between a given memory level and the cores (e.g., DRAM <-> SM). Units: bytes/s\n",
    "    - We will refer to achieved FLOPs/sec as $T$ for throughput, and the upper bound on $T$ as $T_{max}$\n",
    "    - The maximum FLOP/sec we can achieve ($T_{max}$) is modeled as a function of a variable called *computational intensity* ($I$), this is a property of the algorithm we will write.\n",
    "        - Units: FlOP/byte\n",
    "        - FLOPs done per byte moved between that memory level and the cores (reads + writes).\n",
    "    - This metric measures the \"data reuse\" of our algorithm in units of FLOPs/byte'\n",
    "        - For each byte moved from slow memory to fast memory, how many FLOPs do we perform on it.\n",
    "    - The roofline model says the upper bound on FLOPs/sec ($T_{max}$) we can achieve is the minimum of our computational intensity times memory bandwidth, and the peak floating point throughput of our hardware\n",
    "                                        $T_{max} = min(\\beta * I, \\tau)$\n",
    "    - The roofline model says there are two ways $T_{max}$ can be limited:\n",
    "        - $T_{max}$ can never exceed $\\tau$. Even if we perform infinity operations on each byte we move into fast memory, we are still limited by the peak floating point throughput of our hardware.\n",
    "            - When $\\tau$ is our limiting factor we are *compute-bound*, this is a great place to be.\n",
    "        - $T_{max}$ may also be limited by our memory bandwidth times the computational intensity of our algorithm. If $\\tau$ were infinite, the achieved floating point throughput would simply be the number of bytes/sec being moved into fast memory, times the number of FLOPs performed per byte moved $\\beta * I$\n",
    "            - When we multiply $\\beta$ and $I$, the units cancel out to give FLOP/sec.\n",
    "                - $(bytes/s) *(FLOP/bytes) ={FLOP/s}$\n",
    "            - If $\\beta * I <{\\tau}$, or $I <{\\tau/\\beta}$ then we are *memory-bound*, meaning we are limited by how fast we can feed our compute units.\n",
    "            - In this situation we should rewrite our algorithms to increase *I* in order to make our algorithm compute-bound\n",
    "        - Why do we try to increase $I$ and not our bandwidth?\n",
    "            - Since $\\tau$ and $\\beta$ are limited (they are fixed by hardware), we increase $I$ in order to become compute-bound. \n",
    "                - Otherwise, we would have to change hardware with higher bandwidth.\n",
    "                - Additionaly this approach is better because if our algorithm can maximize *computational intensity* on hardware with less bandwidth then it will perform better on new hardware than an algorithm that doesn't maximize *computational intensity*.\n",
    "        - The ridge point(where the sloped line-> memory-bound meets the flat line-> compute-bound) $I ={\\tau/\\beta}$.\n",
    "    - How do we maximize *computational intensity*?\n",
    "        - In practice, this means moving a chunk of data from slow memory to fast memory, and then performing as many useful operations on it as allowed by our algorithm.\n",
    "        - Maximizing the amount of operations on a chunk of data, means we use that data until it won't be used in an operation again. I.e after we fetch it the first time, we won't fetch it again.\n",
    "        - As a result, we reduce the number of trips to slow memory, and now our performance depends on how many operations we can perform on each byte that is moved into fast memory **compute-bound**\n",
    "\n",
    "- **TL;DR:**\n",
    "    - **Fast memory** (shared memory in the SM) is physically close to the compute units.\n",
    "    - **Slow memory** (DRAM) is farther away, so accessing it takes longer.\n",
    "    - Peak compute **τ (FLOP/s)** is achievable only when your arithmetic intensity **I** is high enough and the cores are kept busy\n",
    "    - Achievable performance: $T = min(\\beta * I, \\tau)$; To beat the memory wall, reduce DRAM traffic and increase reuse, so $I >{\\tau/\\beta}$, keeping work in **fast memory**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7ef28",
   "metadata": {},
   "source": [
    "## Rooflines for NVIDIA Tesla T4\n",
    "- We will plug in some numbers specific to our GPU, and look at the resulting roofline model to inform us on how to approach designing our algorithm.\n",
    "    - On a real computer, there isn't just a single $\\tau$ or $\\beta$.\n",
    "    - There are multiple compute ceilings ($\\tau$) for different *instruction paths/data types* (FFMA FP32 vs HMMA FP16/BF16) and multiple bandwidth ceilings ($\\beta$) for different *memory levels* (HBM/DRAM, L2, L1 cache, share memory.)\n",
    "\n",
    "\n",
    "### Tensor Core vs. FFMA\n",
    "\n",
    "- **Tensor Cores** are NVIDIA's specialized hardware unit designed for matrix multiply-accumulate (MMA). \n",
    "    - It computes a small tile operation like $C_{tile} +={A_{tile} \\times B_{tile}}$ in one instruction at **warp scope** (more on this later)\n",
    "    - Instead of doing scalar operations one-by-one on CUDA cores, a Tensor Core performs many fused multiply-adds in parallel on fixed-size tiles\n",
    "\n",
    "- **FFMA** (Fused Floating Multiply-Add)\n",
    "    - A single instruction on CUDA cores that computes $d ={a \\times b + c}$ with one rounding at the end.\n",
    "        - One rounding (fused) means less rounding error than separate multiplication then addition.\n",
    "\n",
    "### Side-by-side example (4×4 matrix)\n",
    "\n",
    "Suppose we want to multiply two $4 \\times 4$ matrices $A$ and $B$ and accumulate into $C$.\n",
    "\n",
    "- **Using only FFMAs (CUDA cores):**\n",
    "  - Each element of $C$ is a dot product of one row of $A$ and one column of $B$:  \n",
    "\n",
    "    $\n",
    "    C[i,j] = \\sum_{k=0}^{3} A[i,k] \\times B[k,j]\n",
    "    $\n",
    "\n",
    "  - Each dot product has **4 multiply–adds**, so computing one $C[i,j]$ requires **4 FFMAs**.  \n",
    "  - Since $C$ is $4 \\times 4$, it has **16 elements total**.  \n",
    "  - Therefore the total work is  \n",
    "\n",
    "    $\n",
    "    16 \\times 4 = 64 \\;\\; \\text{FFMAs across the whole matrix.}\n",
    "    $\n",
    "\n",
    "  - Each FFMA is of the form $d = a \\times b + c$, updating one scalar at a time.\n",
    "\n",
    "- **Using one Tensor Core instruction (HMMA):**\n",
    "  - Instead of 64 separate scalar instructions, the entire warp issues a single **HMMA instruction** that updates the whole $4 \\times 4$ tile of $C$ at once:  \n",
    "\n",
    "    $\n",
    "    C_{4\\times4} \\mathrel{+{=}} A_{4\\times4} \\times B_{4\\times4}\n",
    "    $\n",
    "\n",
    "  - Under the hood, this one instruction bundles together all 64 multiply–adds required for the tile.  \n",
    "  - To the programmer, it’s **one warp-level instruction** instead of 64 separate scalar FFMAs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e3e91",
   "metadata": {},
   "source": [
    "- In order to design our roofline model, we first need to know the global memory bandwidth $\\beta_{gmem}$ of our device. \n",
    "    - NVIDIA spec sheets report *theoretical* memory bandwidth, which is never achievable in practice. So instead, we use a benchmark.\n",
    "    - According to [\"Dissecting the NVidia Turing T4 GPU via Microbenchmarking\"](https://arxiv.org/pdf/1903.07486), the achievable memory bandwidth of the T4 is 220 GB/sec (this is 68% of the 320 GB/sec theoretical memory bandwidth)\n",
    "\n",
    "- Next, we look at the peak floating point throughput with and without the tensor core.\n",
    "    - Similarly to memory, the theoretical numbers are not achievable without.\n",
    "    - Instead we use cuBLAS (matrix multiplication library) half precision and single precision GEMM kernels as the achievable floating point throughput numbers.\n",
    "        - half precision uses **tensor cores** while single precision doesn't\n",
    "    - The half precision kernel is done by **HMMA.1688**\n",
    "        - This instruction performs a single small hardware accelerated matmul\n",
    "    - The single precision kernel is done by **FFMA**\n",
    "    - According to the benchmarks obtained by Alex, the tensor core **HMMA.1688** throughput is 49439 GFLOP/sec, which we will call $\\tau_{HMMA}$.\n",
    "    - The non-tensor core FFMA throughput is 7455 GFLOP/sec which we will call $\\tau_{FFMA}$\n",
    "    - These are respectively 76% and 92% of the theoretical peak throughputs\n",
    "        - HMMA is 76% of Tensor-core theoretical peak\n",
    "        - FFMA is 92% of CUDA-core theoretical peak\n",
    "    - We get the resulting roofline model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2380e",
   "metadata": {},
   "source": [
    "![image.png](../../images/GEMM2/t4_roofline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29369b2",
   "metadata": {},
   "source": [
    "- From the plot it is clear that the comparative hardness of writing a kernel that achieves peak FLOP/sec with tensor core instructions is harder than with fused multiply add instructions\n",
    "    - This comes from the fact that the peak throughput of tensor core $\\tau_{HMMA}$ needs ~6.6x more arithmetic intensity than what we need for peak throughput for fused multiply add $\\tau_{FFMA}$\n",
    "    - The balance points indicate that with FFMA instructions we can perform ~33 FLOPs per byte fetched from DRAM, whereas with the tensor cores we can perform ~224 FLOPs per byte fetched from DRAM.\n",
    "        - This means if we took a kernel that reached peak flops achievable with FFMA instructions, simply replacing the fused multiply adds in the inner loop with tensor core instructions would not be sufficient enough to get high tensor core utilization.\n",
    "        - We would additionally need to improve the code that moves data around to increase computational intensity by a factor of six.\n",
    "\n",
    "### Shared memory vs. L2 cache vs. global memory\n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
