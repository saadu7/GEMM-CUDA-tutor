{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c36659c",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## Memory Wall\n",
    "\n",
    "- Every few years the number of transistors on a microchip increases, and as a result the capacity for performing arithmetic operations has increased exponentially.\n",
    "\n",
    "    - A **transistor** is a tiny electronic switch — the basic “work unit” of digital logic and memory. Modern CPUs and GPUs have billions of them.\n",
    "\n",
    "    - More transistors enable more logic gates, memory cells, and specialized units → greater parallelism, higher throughput, and more complex computations.\n",
    "\n",
    "- However, data movement capacity **(memory bandwidth)** hasn’t increased as fast, creating the **memory wall** — a key bottleneck in deep learning and Tensor Core workloads.\n",
    "    - Bandwidth is how much **data per second** your memory system can deliver to (and accept from) the **compute units**\n",
    "    - **Analogy**:\n",
    "        - Think of a highawy:\n",
    "            - Cars = **bytes**\n",
    "            - Number of lanes = **bus width**\n",
    "            - Speed limit = **clock rate**\n",
    "            - Toll booths/merges = **memory controllers/channels**\n",
    "        - How many cars reach the city each second? That's the bandwidth\n",
    "    - Do not confuse **bandwidth** with **latency**:\n",
    "        - **Latency** is how long one trip takes\n",
    "        - **Bandwidth** is how many bytes per second you can keep flowing\n",
    "\n",
    "- Hence if we want to fully utilize Tensor Cores, we must increase the number of bytes moved between DRAM and compute units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374847f",
   "metadata": {},
   "source": [
    "![image.png](../../images/GEMM2/simple_computer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4b4c9",
   "metadata": {},
   "source": [
    "## Roofline Charts \n",
    "\n",
    "- The roofline model states, that performance will be limited by **one of two things**\n",
    "    - **Compute:**\n",
    "        - Data is readily available in fast memory\n",
    "        - The bottleneck is the number of floating-point operations per second (**FLOP/s)** your hardware can execute\n",
    "        - Adding more **bandwidth** here won’t help — only faster **ALUs (Arithmetic Logical Unit: Used to perform computations)**, **Tensor Cores**, or more **parallelism** will improve performance\n",
    "    - **Memory-bound:**\n",
    "        - The **compute units can’t be** **fed fast enough** because data has to be fetched from slow memory (**DRAM**)\n",
    "        - The bottleneck is **memory bandwidth (β bytes/sec)**\n",
    "        - Doubling **compute** units won’t help unless you also increase your **bandwidth** or improve data reuse in fast memory.\n",
    "    - Plot Interpretation:\n",
    "        - **X-axis:** Operation intensity = **FLOPs** per byte transferred (how much work you do per data fetched)\n",
    "        - **Y-axis:** Achievable performance (**FLOP/s)**\n",
    "        - The **roof** has two segments:\n",
    "            - **Slopped line →** memory-bound region\n",
    "            - **Flat line** → compute-bound region\n",
    "            - Where you land depends on how much computation you can do before you have to fetch more data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9614c835",
   "metadata": {},
   "source": [
    "![image.png](../../images/GEMM2/roof_line.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c57ff",
   "metadata": {},
   "source": [
    "- **Explanation**\n",
    "    - Any given computation has a certain number of FLOPs that need to be performed. For example, if you want to multiply a M by K matrix with a K by N matrix we need to perform $2 * M * N * K$ FLOPs.\n",
    "        - For each pair $(i, j)$ you do K multiplies, and K - 1 adds $\\approx{2K FLOPS}$. 1 FLOP(Floating point operation) for multiply + 1 FLOP for addition\n",
    "        - There are M * N outputs, so total $FLOPs \\approx{(M N) * (2K)} ={2*M*N*K}$\n",
    "    - The more FLOPs/sec our algorithm can achieve, the faster we can get the matrix multiplication done.\n",
    "    - The roofline model gives us an upper bound on the FLOPs/sec we can achieve, subject to $\\tau$ and $\\beta$ which are fixed properties of our hardware.\n",
    "        - $\\tau$ (tau) = the **peak compute throughput** of your device for a given datatype/op (e.g **FP32** (Floating Point 32-bit float), **FP16/Tensor**)\n",
    "            - $\\tau$ is typically a large number. For example, for the T4 GPU, $\\tau$= 65,000,000,000,000 FLOPs/second. Units: FLOP/s\n",
    "        - $\\beta$ (beta) = the **peak sustained memory bandwidth** between a given memory level and the cores (e.g., DRAM <-> SM). Units: bytes/s\n",
    "    - We will refer to achieved FLOPs/sec as $T$ for throughput, and the upper bound on $T$ as $T_{max}$\n",
    "    - The maximum FLOP/sec we can achieve ($T_{max}$) is modeled as a function of a variable called *computational intensity* ($I$), this is a property of the algorithm we will write.\n",
    "        - Units: FlOP/byte\n",
    "        - FLOPs done per byte moved between that memory level and the cores (reads + writes).\n",
    "    - This metric measures the \"data reuse\" of our algorithm in units of FLOPs/byte'\n",
    "        - For each byte moved from slow memory to fast memory, how many FLOPs do we perform on it.\n",
    "    - The roofline model says the upper bound on FLOPs/sec ($T_{max}$) we can achieve is the minimum of our computational intensity times memory bandwidth, and the peak floating point throughput of our hardware\n",
    "                                        $T_{max} = min(\\beta * I, \\tau)$\n",
    "    - The roofline model says there are two ways $T_{max}$ can be limited:\n",
    "        - $T_{max}$ can never exceed $\\tau$. Even if we perform infinity operations on each byte we move into fast memory, we are still limited by the peak floating point throughput of our hardware.\n",
    "            - When $\\tau$ is our limiting factor we are *compute-bound*, this is a great place to be.\n",
    "        - $T_{max}$ may also be limited by our memory bandwidth times the computational intensity of our algorithm. If $\\tau$ were infinite, the achieved floating point throughput would simply be the number of bytes/sec being moved into fast memory, times the number of FLOPs performed per byte moved $\\beta * I$\n",
    "            - When we multiply $\\beta$ and $I$, the units cancel out to give FLOP/sec.\n",
    "                - $(bytes/s) *(FLOP/bytes) ={FLOP/s}$\n",
    "            - If $\\beta * I <{\\tau}$, or $I <{\\tau/\\beta}$ then we are *memory-bound*, meaning we are limited by how fast we can feed our compute units.\n",
    "            - In this situation we should rewrite our algorithms to increase *I* in order to make our algorithm compute-bound\n",
    "        - Why do we try to increase $I$ and not our bandwidth?\n",
    "            - Since $\\tau$ and $\\beta$ are limited (they are fixed by hardware), we increase $I$ in order to become compute-bound. \n",
    "                - Otherwise, we would have to change hardware with higher bandwidth.\n",
    "                - Additionaly this approach is better because if our algorithm can maximize *computational intensity* on hardware with less bandwidth then it will perform better on new hardware than an algorithm that doesn't maximize *computational intensity*.\n",
    "        - The ridge point(where the sloped line-> memory-bound meets the flat line-> compute-bound) $I ={\\tau/\\beta}$.\n",
    "    - How do we maximize *computational intensity*?\n",
    "        - In practice, this means moving a chunk of data from slow memory to fast memory, and then performing as many useful operations on it as allowed by our algorithm.\n",
    "        - Maximizing the amount of operations on a chunk of data, means we use that data until it won't be used in an operation again. I.e after we fetch it the first time, we won't fetch it again.\n",
    "        - As a result, we reduce the number of trips to slow memory, and now our performance depends on how many operations we can perform on each byte that is moved into fast memory **compute-bound**\n",
    "\n",
    "- **TL;DR:**\n",
    "    - **Fast memory** (shared memory in the SM) is physically close to the compute units.\n",
    "    - **Slow memory** (DRAM) is farther away, so accessing it takes longer.\n",
    "    - Peak compute **τ (FLOP/s)** is achievable only when your arithmetic intensity **I** is high enough and the cores are kept busy\n",
    "    - Achievable performance: $T = min(\\beta * I, \\tau)$; To beat the memory wall, reduce DRAM traffic and increase reuse, so $I >{\\tau/\\beta}$, keeping work in **fast memory**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7ef28",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
