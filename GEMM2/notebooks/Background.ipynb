{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c36659c",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## Memory Wall\n",
    "\n",
    "- Every few years the number of transistors on a microchip increases, and as a result the capacity for performing arithmetic operations has increased exponentially.\n",
    "\n",
    "    - A **transistor** is a tiny electronic switch — the basic “work unit” of digital logic and memory. Modern CPUs and GPUs have billions of them.\n",
    "\n",
    "    - More transistors enable more logic gates, memory cells, and specialized units → greater parallelism, higher throughput, and more complex computations.\n",
    "\n",
    "- However, data movement capacity **(memory bandwidth)** hasn’t increased as fast, creating the **memory wall** — a key bottleneck in deep learning and Tensor Core workloads.\n",
    "    - Bandwidth is how much **data per second** your memory system can deliver to (and accept from) the **compute units**\n",
    "    - **Analogy**:\n",
    "        - Think of a highawy:\n",
    "            - Cars = **bytes**\n",
    "            - Number of lanes = **bus width**\n",
    "            - Speed limit = **clock rate**\n",
    "            - Toll booths/merges = **memory controllers/channels**\n",
    "        - How many cars reach the city each second? That's the bandwidth\n",
    "    - Do not confuse **bandwidth** with **latency**:\n",
    "        - **Latency** is how long one trip takes.\n",
    "            - When you fetch datam the **first byte** doesn't arrive immediately, there's a startup delay. That delay is your **latency** (i.e., how long it takes the first byte to show up).\n",
    "        - **Bandwidth** is how many bytes per second you can keep flowing\n",
    "            - After the pipeline is primed (first byte arrives), the system can deliver a **continuous stream of bytes** every cycle. That steady flow rate is your **bandwidth** (i.e, how much data is moving per unit time).\n",
    "\n",
    "- Hence if we want to fully utilize Tensor Cores, we must increase the number of bytes moved between DRAM and compute units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374847f",
   "metadata": {},
   "source": [
    "![image.png](../../images/GEMM2/simple_computer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4b4c9",
   "metadata": {},
   "source": [
    "## Roofline Charts \n",
    "\n",
    "- The roofline model states, that performance will be limited by **one of two things**\n",
    "    - **Compute:**\n",
    "        - Data is readily available in fast memory\n",
    "        - The bottleneck is the number of floating-point operations per second (**FLOP/s)** your hardware can execute\n",
    "        - Adding more **bandwidth** here won’t help — only faster **ALUs (Arithmetic Logical Unit: Used to perform computations)**, **Tensor Cores**, or more **parallelism** will improve performance\n",
    "    - **Memory-bound:**\n",
    "        - The **compute units can’t be** **fed fast enough** because data has to be fetched from slow memory (**DRAM**)\n",
    "        - The bottleneck is **memory bandwidth (β bytes/sec)**\n",
    "        - Doubling **compute** units won’t help unless you also increase your **bandwidth** or improve data reuse in fast memory.\n",
    "    - Plot Interpretation:\n",
    "        - **X-axis:** Operation intensity = **FLOPs** per byte transferred (how much work you do per data fetched)\n",
    "        - **Y-axis:** Achievable performance (**FLOP/s)**\n",
    "        - The **roof** has two segments:\n",
    "            - **Slopped line →** memory-bound region\n",
    "            - **Flat line** → compute-bound region\n",
    "            - Where you land depends on how much computation you can do before you have to fetch more data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9614c835",
   "metadata": {},
   "source": [
    "![image.png](../../images/GEMM2/roof_line.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c57ff",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "- Any given computation has a certain number of FLOPs that need to be performed. For example, if you want to multiply a M by K matrix with a K by N matrix we need to perform $2 * M * N * K$ FLOPs.\n",
    "    - For each pair $(i, j)$ you do K multiplies, and K - 1 adds $\\approx{2K FLOPS}$. 1 FLOP(Floating point operation) for multiply + 1 FLOP for addition\n",
    "    - There are M * N outputs, so total $FLOPs \\approx{(M N) * (2K)} ={2*M*N*K}$\n",
    "- The more FLOPs/sec our algorithm can achieve, the faster we can get the matrix multiplication done.\n",
    "- The roofline model gives us an upper bound on the FLOPs/sec we can achieve, subject to $\\tau$ and $\\beta$ which are fixed properties of our hardware.\n",
    "    - $\\tau$ (tau) = the **peak compute throughput** of your device for a given datatype/op (e.g **FP32** (Floating Point 32-bit float), **FP16/Tensor**)\n",
    "        - $\\tau$ is typically a large number. For example, for the T4 GPU, $\\tau$= 65,000,000,000,000 FLOPs/second. Units: FLOP/s -> 65 TFLOPs/second -> Tera FLOPs = $10^9$ FLOPs\n",
    "    - $\\beta$ (beta) = the **peak sustained memory bandwidth** between a given memory level and the cores (e.g., DRAM <-> SM). Units: bytes/s\n",
    "- We will refer to achieved FLOPs/sec as $T$ for throughput, and the upper bound on $T$ as $T_{max}$\n",
    "- The maximum FLOP/sec we can achieve ($T_{max}$) is modeled as a function of a variable called *computational intensity* ($I$), this is a property of the algorithm we will write.\n",
    "    - *Computational Intensity*: FLOPs done per byte moved between that memory level and the cores (reads + writes).\n",
    "        - Units: FLOP/byte\n",
    "- This metric measures the \"data reuse\" of our algorithm\n",
    "    - For each byte moved from slow memory to fast memory, how many FLOPs do we perform on it.\n",
    "- The roofline model says the upper bound on FLOPs/sec, ($T_{max}$), we can achieve, is the minimum of our computational intensity times memory bandwidth, and the peak floating point throughput of our hardware\n",
    "                                    \n",
    "    $\n",
    "    T_{max} = \\min(\\beta \\cdot I, \\tau)\n",
    "    $\n",
    "\n",
    "- The ridge point (where the sloped line → memory-bound meets the flat line → compute-bound) is given by:\n",
    "\n",
    "    $\n",
    "    I^* = \\frac{\\tau}{\\beta}\n",
    "    $\n",
    "\n",
    "- The roofline model says there are two ways $T_{max}$ can be limited:\n",
    "    - $T_{max}$ can never exceed $\\tau$. Even if we perform infinity operations on each byte we move into fast memory, we are still limited by the peak floating point throughput of our hardware.\n",
    "        - When $\\tau$ is our limiting factor we are *compute-bound*, this is a great place to be.\n",
    "    - $T_{max}$ may also be limited by our memory bandwidth times the computational intensity of our algorithm. If $\\tau$ were infinite, the achieved floating point throughput would simply be the number of bytes/sec being moved into fast memory, times the number of FLOPs performed per byte moved $\\beta * I$\n",
    "        - When we multiply $\\beta$ and $I$, the units cancel out to give FLOP/sec.\n",
    "            - $(bytes/s) *(FLOP/bytes) ={FLOP/s}$\n",
    "        - If $\\beta * I <{\\tau}$, or $I <{\\tau/\\beta}$ then we are *memory-bound*, meaning we are limited by how fast we can feed our compute units.\n",
    "        - In this situation we should rewrite our algorithms to increase *computational intensity* $I$ in order to make our algorithm compute-bound\n",
    "    - Why do we try to increase $I$ and not our bandwidth?\n",
    "        - Since $\\tau$ and $\\beta$ are limited (they are fixed by hardware), we increase $I$ in order to become compute-bound. \n",
    "            - Otherwise, we would have to change to hardware with higher bandwidth.\n",
    "            - Additionaly this approach is better because if our algorithm can maximize *computational intensity* on hardware with less bandwidth then it will perform better on new hardware than an algorithm that doesn't maximize *computational intensity*.\n",
    "- How do we maximize *computational intensity*?\n",
    "    - In practice, this means moving a chunk of data from slow memory to fast memory, and then performing as many useful operations on it as allowed by our algorithm.\n",
    "    - Maximizing the amount of operations on a chunk of data, means we use that data until it won't be used in an operation again. I.e after we fetch it the first time, we won't fetch it again.\n",
    "    - As a result, we reduce the number of trips to slow memory, and now our performance depends on how many operations we can perform on each byte that is moved into fast memory -> we are **compute-bound**\n",
    "\n",
    "### TL;DR\n",
    "- **Fast memory** (shared memory in the SM) is physically close to the compute units.\n",
    "- **Slow memory** (DRAM) is farther away, so accessing it takes longer.\n",
    "- Peak compute **τ (FLOP/s)** is achievable only when your arithmetic intensity **I** is high enough and the cores are kept busy\n",
    "- Achievable performance: $T = min(\\beta * I, \\tau)$; To beat the memory wall, reduce DRAM traffic and increase reuse, so $I >{\\tau/\\beta}$, keeping work in **fast memory**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7ef28",
   "metadata": {},
   "source": [
    "## Rooflines for NVIDIA Tesla T4\n",
    "- We will plug in some numbers specific to our GPU, and look at the resulting roofline model to inform us on how to approach designing our algorithm.\n",
    "    - On a real computer, there isn't just a single $\\tau$ or $\\beta$.\n",
    "    - There are multiple compute ceilings ($\\tau$) for different *instruction paths/data types* (FFMA FP32 vs HMMA FP16/BF16) and multiple bandwidth ceilings ($\\beta$) for different *memory levels* (HBM/DRAM, L2, L1 cache, share memory.)\n",
    "\n",
    "\n",
    "### Tensor Core vs. FFMA\n",
    "\n",
    "- **Tensor Cores** are NVIDIA's specialized hardware unit designed for matrix multiply-accumulate (MMA). \n",
    "    - It computes a small tile operation like $C_{tile} +={A_{tile} \\times B_{tile}}$ in one instruction at **warp scope** (more on this later)\n",
    "    - Instead of doing scalar operations one-by-one on CUDA cores, a Tensor Core performs many fused multiply-adds in parallel on fixed-size tiles\n",
    "\n",
    "- **FFMA** (Fused Floating Multiply-Add)\n",
    "    - A single instruction on CUDA cores that computes $d ={a \\times b + c}$ with one rounding at the end.\n",
    "        - One rounding (fused) means less rounding error than separate multiplication then addition.\n",
    "\n",
    "### Side-by-side example (4×4 matrix)\n",
    "\n",
    "Suppose we want to multiply two $4 \\times 4$ matrices $A$ and $B$ and accumulate into $C$.\n",
    "\n",
    "- **Using only FFMAs (CUDA cores):**\n",
    "  - Each element of $C$ is a dot product of one row of $A$ and one column of $B$:  \n",
    "\n",
    "    $\n",
    "    C[i,j] = \\sum_{k=0}^{3} A[i,k] \\times B[k,j]\n",
    "    $\n",
    "\n",
    "  - Each dot product has **4 multiply–adds**, so computing one $C[i,j]$ requires **4 FFMAs**.  \n",
    "  - Since $C$ is $4 \\times 4$, it has **16 elements total**.  \n",
    "  - Therefore the total work is  \n",
    "\n",
    "    $\n",
    "    16 \\times 4 = 64 \\;\\; \\text{FFMAs across the whole matrix.}\n",
    "    $\n",
    "\n",
    "  - Each FFMA is of the form $d = a \\times b + c$, updating one scalar at a time.\n",
    "\n",
    "- **Using one Tensor Core instruction (HMMA):**\n",
    "  - Instead of 64 separate scalar instructions, the entire warp issues a single **HMMA instruction** that updates the whole $4 \\times 4$ tile of $C$ at once:  \n",
    "\n",
    "    $\n",
    "    C_{4\\times4} \\mathrel{+{=}} A_{4\\times4} \\times B_{4\\times4}\n",
    "    $\n",
    "\n",
    "  - Under the hood, this one instruction bundles together all 64 multiply–adds required for the tile.  \n",
    "  - To the programmer, it’s **one warp-level instruction** instead of 64 separate scalar FFMAs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e3e91",
   "metadata": {},
   "source": [
    "- In order to design our roofline model, we first need to know the global memory bandwidth $\\beta_{gmem}$ of our device. \n",
    "    - NVIDIA spec sheets report *theoretical* memory bandwidth, which is never achievable in practice. So instead, we use a benchmark.\n",
    "    - According to [\"Dissecting the NVidia Turing T4 GPU via Microbenchmarking\"](https://arxiv.org/pdf/1903.07486), the achievable memory bandwidth of the T4 is 220 GB/sec (this is 68% of the 320 GB/sec theoretical memory bandwidth)\n",
    "\n",
    "- Next, we look at the peak floating point throughput with and without the tensor core.\n",
    "    - Similarly to memory, the theoretical numbers are not achievable without.\n",
    "    - Instead we use cuBLAS (matrix multiplication library) half precision and single precision GEMM kernels as the achievable floating point throughput numbers.\n",
    "        - half precision uses **tensor cores** while single precision doesn't\n",
    "    - The half precision kernel is done by **HMMA.1688**\n",
    "        - This instruction performs a single small hardware accelerated matmul\n",
    "    - The single precision kernel is done by **FFMA**\n",
    "    - According to the benchmarks obtained by Alex, the tensor core **HMMA.1688** throughput is 49439 GFLOP/sec, which we will call $\\tau_{HMMA}$.\n",
    "    - The non-tensor core FFMA throughput is 7455 GFLOP/sec which we will call $\\tau_{FFMA}$\n",
    "    - These are respectively 76% and 92% of the theoretical peak throughputs\n",
    "        - HMMA is 76% of Tensor-core theoretical peak\n",
    "        - FFMA is 92% of CUDA-core theoretical peak\n",
    "    - We get the resulting roofline model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2380e",
   "metadata": {},
   "source": [
    "![image.png](../../images/GEMM2/t4_roofline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29369b2",
   "metadata": {},
   "source": [
    "- From the plot it is clear that the comparative hardness of writing a kernel that achieves peak FLOP/sec with tensor core instructions is harder than with fused multiply add instructions\n",
    "    - This comes from the fact that the peak throughput of tensor core $\\tau_{HMMA}$ needs ~6.6x more arithmetic intensity than what we need for peak throughput for fused multiply add $\\tau_{FFMA}$\n",
    "    - The balance points indicate that with FFMA instructions we can perform ~33 FLOPs per byte fetched from DRAM, whereas with the tensor cores we can perform ~224 FLOPs per byte fetched from DRAM.\n",
    "        - This means if we took a kernel that reached peak flops achievable with FFMA instructions, simply replacing the fused multiply adds in the inner loop with tensor core instructions would not be sufficient enough to get high tensor core utilization.\n",
    "        - We would additionally need to improve the code that moves data around to increase computational intensity by a factor of six.\n",
    "\n",
    "### Shared memory vs. L2 cache vs. global memory\n",
    "- It is crucial to understand our computers memory hierarchy if we want to write an optimized kernel for the tensor cores.\n",
    "    - The roofline model simplifies the memory hierarchy down to two storage types, one large and slow, and the other fast and instantaneous.\n",
    "    - In reality, there are more memory levels, each with a different bandwidth and capacity.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115d5dca",
   "metadata": {},
   "source": [
    "![image.png](../../images/GEMM2/t4_memory_hierarchy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f33dc0",
   "metadata": {},
   "source": [
    "- It is critical to use the faster and smaller levels of the memory hierarchy effectively in order to increase **arithmetic intensity**, and thus move from being memory-bound to compute-bound.\n",
    "    - This requires ingenuity because of the limited size of on-chip memory. For instance, on the Tesla T4 the **shared memory** has about **16.6× the bandwidth of global (DRAM) memory**.\n",
    "    - However, on each streaming multiprocessor (SM) it only fits **64 KiB**.  \n",
    "        - **KiB (kibibyte) = 1024 bytes**\n",
    "    - When multiplying large matrices, 64 KiB is only enough to fit a **tiny tile of A, B, and C** at once. Efficient kernels must therefore reuse these tiles heavily before loading new ones from global memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c5a62a",
   "metadata": {},
   "source": [
    "![image.png](../../images/GEMM2/t4_memory_roofline1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d721f8",
   "metadata": {},
   "source": [
    "The plot compares the balance point of tensor cores with respect to:\n",
    "- **Global memory (DRAM)**\n",
    "    - Largest and slowest level of the memory hierarchy\n",
    "- **L2 Cache**\n",
    "    - Stores recently accessed data from DRAM, and is shared between the 16 SMs on the T4\n",
    "- **Shared Memory**\n",
    "    - Memory on SM\n",
    "\n",
    "- All of these balance points are with respect to the tensor cores.\n",
    "- Global memory has a balance point of 224, this means we need 224 FLOPs per byte fetched from DRAM in order to keep our tensor cores busy.\n",
    "- The L2 cache has a balance point of 38, which is much more of a manageable number\n",
    "    - If a good number of our memory accesses can hit the L2 cache rather than going all the way to global memory, we are more likely to become compute bound.\n",
    "    - Why not try and get memory access to hit shared memory?\n",
    "        - Shared memory isn't large enough to hold the full matrices, so we use it to store tiles of the matrices instead.\n",
    "- Instead shared memory is used to explicitly manage cache that will hold small portions of the input matrices local to a particular SM.\n",
    "- Within the SM, threads will load their own local portion of the matrices from shared memory into register memory\n",
    "    - Register memory is where data must reside in order for it to be computed on. \n",
    "    - I.e, if we want to perform an add (a + b), we retrieve the values from shared memory, then store them on registers. Compute the sum, and store it back to one of the registers. Then eventually write it back to global memory.\n",
    "- When shared memory is operating at full bandwidth, its balance point is 13. \n",
    "    - This means we need to cache enough data in registers to perform 13 FLOPs for each byte read from shared memory.\n",
    "    - The SMs have enough register memory, which allows us to do that.\n",
    "- Our challenge will be to enable shared memory to operate at full bandwidth\n",
    "    - In practice this means organizing the data layout in such a way that we can read it and write it without bank conflicts.\n",
    "- Once shared memory is at full bandwidth, sufficient arithmetic intensity will be easy to achieve.\n",
    "- However, despite the balance point of shared memory being 13, it alone is not fast enough to achieve peak tensor core throughput.\n",
    "    - If we stopped at shared memory reuse, you hit a ceiling: the Tensor Cores can still execute faster than shared memory can feed them.\n",
    "    - So we bring in registers, as they are much faster than shared memory.\n",
    "    - This is possible because each SM on the T4 has **tens of thousands of registers** (65,536 in total), and each thread can use hundreds of them. Registers are the fastest memory on the GPU, with massive aggregate bandwidth across all threads. This makes them capable of reusing values many times and feeding the Tensor Cores at full speed, unlike shared memory which would eventually become a bottleneck.\n",
    "\n",
    "- These balance point numbers (224, 38, 13) all come from the formula:\n",
    "\n",
    "  $\n",
    "  I^* = \\frac{\\tau}{\\beta}\n",
    "  $\n",
    "\n",
    "  where $\\tau$ is the compute throughput of the Tensor Cores and $\\beta$ is the memory bandwidth of that level of the hierarchy.\n",
    "\n",
    "- For the T4, the achievable Tensor Core throughput is about $\\tau_{HMMA}$ = 49,439 GFLOPs  \n",
    "  - If we divide this by the DRAM bandwidth (220 GB/s), we get the global memory balance point $\\approx 224$ FLOPs/byte.  \n",
    "  - Dividing by the L2 bandwidth gives $\\approx 38$ FLOPs/byte.  \n",
    "  - Dividing by the shared memory bandwidth gives $\\approx 13$ FLOPs/byte.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c4121",
   "metadata": {},
   "source": [
    "## Theoretical arithmetic intensity\n",
    "\n",
    "- Modern computers have an imbalance between their arithmetic throughput and their memory bandwidth\n",
    "    - Consequently kernels that perform lots of arithmetic relative to data movement make better use of the hardware\n",
    "\n",
    "### Matrix Multiplication vs Matrix Addition\n",
    "\n",
    "- Any algorithm has a maximum amount of arithmetic intensity that is possible\n",
    "    - Our goal is to write a kernel that achieves an arithmetic intensity as close to the upper bound as possible.\n",
    "    - Comparing the maximum arithmetic intensity that is achievable when adding two $N$ by $N$ matrices, vs. multiplying them, shows us how different algorithms have different upper bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb206860",
   "metadata": {},
   "source": [
    "![image.png](../../images/GEMM2/multiplication_vs_addition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26864501",
   "metadata": {},
   "source": [
    "### Matrix Multiplication vs Matrix Addition (cont'd)\n",
    "\n",
    "- In the case of matrix addition, computing a single output element requires a single **arithmetic** operation.  \n",
    "  - This means that in our algorithm the amount of data movement and the amount of compute grow in direct proportion: if data movement increases, compute increases at the same scale.  \n",
    "  - If we are adding two $N \\times N$ matrices, the amount of data involved is $O(N^2)$, and the amount of compute required is also $O(N^2)$.  \n",
    "    - So the ratio of **compute to data movement** is  \n",
    "      $$\n",
    "      \\frac{O(N^2)}{O(N^2)} = O(1)\n",
    "      $$\n",
    "      which means matrix addition will typically be **memory-bound** on modern devices.\n",
    "\n",
    "### Breakdown\n",
    "\n",
    "- For $C = A + B$:  \n",
    "  - There are $N \\times N$ elements.  \n",
    "  - Each element requires **one addition**, so the total number of operations is $N^2$.  \n",
    "\n",
    "- To calculate **arithmetic intensity**, we need FLOPs and bytes moved for $A$, $B$, and $C$:  \n",
    "  - Read $A$ once: $N^2$ elements  \n",
    "  - Read $B$ once: $N^2$ elements  \n",
    "  - Write $C$ once: $N^2$ elements  \n",
    "  - Total = $3N^2$ elements = $3N^2 s$ bytes (where $s$ = bytes per element).  \n",
    "\n",
    "- Arithmetic intensity:\n",
    "  $$\n",
    "  \\text{arithmetic intensity} = \\frac{FLOPs}{Bytes} = \\frac{N^2}{3N^2 s} = \\frac{1}{3s} = O(1)\n",
    "  $$\n",
    "\n",
    "- Why is it **memory-bound**?\n",
    "- The roofline model tells us that attainable performance is limited by  \n",
    "  $$\n",
    "  T={\\min{(\\tau, \\beta \\cdot I)}} = {\\min{(\\tau, \\beta \\cdot \\text{arithmetic intensity)}}}\n",
    "  $$\n",
    "- If arithmetic intensity is high, the kernel can be **compute-bound**. If it is low, it will be **memory-bound**.  \n",
    "- Since arithmetic intensity for addition is constant and very small, there is no way to grow into the compute-bound region.  \n",
    "  - For every extra byte moved, you only ever get a fixed, small number of FLOPs.  \n",
    "  - That means the bottleneck is permanently **memory bandwidth**.  \n",
    "  - As a result, performance scales almost linearly with how fast the machine can move data from memory, not with how many FLOPs it can theoretically execute.\n",
    "\n",
    "### Matrix Multiplication\n",
    "\n",
    "- Matrix multiplication on the other hand doesn't face the same problem as addition.\n",
    "    - This is because there is more arithmetic required relative to the problem size.\n",
    "- When multiplying two $N \\times N$ matrices, the amount of data involved is also $O(N^2)$, but the amount of compute required is $O(N^3)$.\n",
    "    - $O(N^3)$ -> $O(N)$ operations per output element, times $O(N^2)$ output elements\n",
    "- So the ratio of compute to data movement is $\\frac{O(N^3)}{O(N^2)} = O(N)$\n",
    "- There is a factor of $N$ more compute required than data movement. The upper bound on arithmetic intensity we can achieve grows with the matrix dimension.\n",
    "    - If we are multiplying significantly large matrices, we should write our algorithms so that they have sufficient arithmetic intensity to be compute bound.\n",
    "\n",
    "### TL;DR\n",
    "- The arithmetic intensity we achieve depends on the kernels we write.  \n",
    "    - It must always be less than or equal to an upper bound imposed by the algorithm our kernel is implementing.  \n",
    "        - For instance, matrix addition has a constant arithmetic intensity. No reuse or blocking can increase it.  \n",
    "- The achieved arithmetic intensity, together with our machine parameters $\\tau$ (peak compute throughput) and $\\beta$ (memory bandwidth), determines whether we are memory-bound or compute-bound.  \n",
    "- If the algorithm’s upper bound on arithmetic intensity allows it, we should optimize our kernel to approach that bound so that it becomes compute-bound rather than memory-bound.  \n",
    "    - For example, matrix multiplication’s arithmetic intensity grows with $N$. By reusing elements of $A$ and $B$ through blocking/tiling, we can increase arithmetic intensity enough to become compute-bound.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bef104",
   "metadata": {},
   "source": [
    "## Achievable airthmetic intensity on a simple computer\n",
    "\n",
    "- When multiplying two $N \\times N$ matrices, the best possible arithmetic intensity is O(N). We will model a kernel on a simple computer (has fast and slow memory) to illustrate how this comes in to play.\n",
    "\n",
    "### Worst Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340e98e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "allocate registers a,b,c in fast memory\n",
    "//We'll use the registers to temporarily hold values from matrices a and b\n",
    "//c will be used as an accumulator for the result of the dot products\n",
    "\n",
    "//i represents the rows of C\n",
    "for i=1...N: //Runs N times\n",
    "    //j represents the columns of C\n",
    "    for j=1...N: //Runs N times\n",
    "        // We reset accumulator before computing C(i,j)\n",
    "        c = 0\n",
    "        //k is the shared dimension of matrix A and B\n",
    "        //We loop k times to find the dot product\n",
    "        for k=1...N: //Runs N times\n",
    "            load A(i,k) into a\n",
    "            load B(k,j) into b\n",
    "            //add to accumulator\n",
    "            c += a * b //2 FLOPS (1 multiply and 1 add)\n",
    "        //Once done, store in C(i, j)\n",
    "        store c into C(i,j) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec039a",
   "metadata": {},
   "source": [
    "- Our first implementation of matrix multiplication, loads each value as soon as we need it, and store each output as soon as we are done.\n",
    "\n",
    "![image.png](../../images/GEMM2/simple_computer_matmul_naive.png)\n",
    "\n",
    "- The arithmetic intensity of this implementation on the simple computer is O(1)\n",
    "    - For each iteration of the inner most loop a single multiply-accumulate is performed\n",
    "    - Only the specific data needed for thqt iterqtion is loqded from slow memory\n",
    "    - Hence there is $O(N^3)$ data movement, and $O(N^3)$ compute. Meaning there is $\\frac{O(N^3)}{O(N^3)}={O(1)}$ arithmetic intensity\n",
    "        - Breakdown:\n",
    "            - There are 3 nested loops, meaning the inner most statement executes: $N \\times N \\times N ={N^3}$\n",
    "                - Each execution does 2 FLOPs (1 multiply + 1 add):\n",
    "                 $2 \\times N^3 ={O(N^3)}$\n",
    "            - Data Movement\n",
    "                - 2 loads per inner loop iteration -> $2N^3$ \n",
    "                - 1 store per (i,j) pair -> $N^2$\n",
    "                - Total = $ 2N^3 + N^2 ={O(N^3)}$ (since $N^3$ is the dominant term as $N \\to{\\infty}$)\n",
    "- This is worse than the ideal by a factor of O(N), and turns out to be the worst case\n",
    "\n",
    "### Best Case\n",
    "\n",
    "- The problem with the first implementation is that we load single elements from fast memory one at a time, only when they are needed\n",
    "    - Only three matrix elements at a time are stored in fast memory (a, b, c)\n",
    "- We can imporve intensity by making better use of fast memory\n",
    "- Imagine if fast memory was large enough to fit A, B, and C in their entirety\n",
    "    - We could allocate sapce in fast memory for C\n",
    "    - Transfer the entire A and B upfront\n",
    "    - Perform the three nested loops with all the data already present in fast memory\n",
    "    - Then store the entire C matrix all at once back to slow memory\n",
    "\n",
    "![image.png](../../images/GEMM2/simple_computer_matmul_best_case.png)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
