{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd623a2c",
   "metadata": {},
   "source": [
    "# Kernel 5: 2D Blocktiling\n",
    "\n",
    "So as mentioned, our objective now is to increase arithmetic intensity - essentially want more work done per load we make.\n",
    "\n",
    "We are going to up the ante with our thread's evolving into multitasking agents, with now each thread computing a whopping 8x8 grid of elements of C. It's come a long way since it's single element days.\n",
    "\n",
    "Let's step through the code first, since I think we are getting familiar with the flow of these 'chunk-based' kernels. Then after we can flesh out some intuition with some further explanation and diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf2f38c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "float threadResults[TM * TN] = {0.0};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c81fbb8",
   "metadata": {},
   "source": [
    "Pretty much the same as before, but now we are storing a mini grid of results for each thread, rather than a single column.\n",
    "\n",
    "__Note: TM and TN are both 8 in our example here__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8ffeb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "float regM[TM] = {0.0};\n",
    "float regN[TN] = {0.0};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e2b8f",
   "metadata": {},
   "source": [
    "Something new here, we use thread-private registers that are the fastest storage we can use.\n",
    "\n",
    "These will store TM elements from As, and TN elements from Bs the compute each inner loops partial-partial dot product. This should make more sense as you see it in action.\n",
    "\n",
    "Now, taking a look at the (very similar) outer loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268dd7a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "// outer loop\n",
    "for (uint bkIdx = 0; bkIdx < K; bkIdx += BK) {\n",
    "  // populate the SMEM caches\n",
    "  for (uint loadOffset = 0; loadOffset < BM; loadOffset += strideA) {\n",
    "    As[(innerRowA + loadOffset) * BK + innerColA] =\n",
    "        A[(innerRowA + loadOffset) * K + innerColA];\n",
    "  }\n",
    "  for (uint loadOffset = 0; loadOffset < BK; loadOffset += strideB) {\n",
    "    Bs[(innerRowB + loadOffset) * BN + innerColB] =\n",
    "        B[(innerRowB + loadOffset) * N + innerColB];\n",
    "  }\n",
    "  __syncthreads();\n",
    "\n",
    "  // ... rest of outer loop body\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8175d92",
   "metadata": {},
   "source": [
    "So, we can see the outer loop is the same, just advancing through the columns of A and rows of B chunk by chunk,\n",
    "\n",
    "However, now the loading into the SMEM caches is wholely different, having seperate loops for these, which we did not use before.\n",
    "\n",
    "This is as each thread will load multiple elements instead of just one, but evidently, it still won't load all the elements it needs, as that is the power of the shared memory - other threads will also load in elements that a thread will need.\n",
    "\n",
    "it will go through both As and Bs, and load in one element per stride, as we slide along the whole of As in strides.\n",
    "\n",
    "Let's take a look at what is happening visually, for As:\n",
    "\n",
    "![](../../images/GEMM1/strideloading.png)\n",
    "\n",
    "Each thread will only compute one entry per cycle (iteration), with each cycle being a sub-chunk of As that moves down by strideA.\n",
    "\n",
    "Similarly, this is what happens what happens along the columns of Bs.\n",
    "\n",
    "If we notice something, even if we assume threads are computing whole 8x8 grids of output entries, they will be loading in entries that span the whole of the current As buffer, some of which will never be used by the thread itself, but it still loads into shared memory for the other threads. \n",
    "\n",
    "This really highlights how the first step of blocktiling is collaboration-focused, with our thread workers being strong in arms.\n",
    "\n",
    "Then the __syncthreads() call, again, just ensures that all the workers are done collaboratively loading into SMEM, and can be aligned to start using the fruits of their labour for dot products.\n",
    "\n",
    "Looking further into the rest of the outer loop body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f549ff4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "  // first inner loop\n",
    "  for (uint dotIdx = 0; dotIdx < BK; ++dotIdx) {\n",
    "    // load relevant As & Bs entries into registers\n",
    "    for (uint i = 0; i < TM; ++i) {\n",
    "      regM[i] = As[(threadRow * TM + i) * BK + dotIdx];\n",
    "    }\n",
    "    for (uint i = 0; i < TN; ++i) {\n",
    "      regN[i] = Bs[dotIdx * BN + threadCol * TN + i];\n",
    "    }\n",
    "    // perform outer product on register cache, accumulate\n",
    "\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8e1b5",
   "metadata": {},
   "source": [
    "For our first inner loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f10ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    // into threadResults\n",
    "    for (uint resIdxM = 0; resIdxM < TM; ++resIdxM) {\n",
    "      for (uint resIdxN = 0; resIdxN < TN; ++resIdxN) {\n",
    "        threadResults[resIdxM * TN + resIdxN] +=\n",
    "            regM[resIdxM] * regN[resIdxN];\n",
    "      }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e238de84",
   "metadata": {},
   "source": [
    "Jeez, the loops keeping on coming. This is still inside the same first inner loop - this is also a good time to remind you about the raw existing code living the kernels/ folder, under the same corresponding name as this .ipynb file, since it is a lot easier to look at the code in its entirety, as it can be jarring to see it in chunks like this, especially with the amount of loops we are seeing.\n",
    "\n",
    "So this first loop is just"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ff3a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "__syncthreads();\n",
    "\n",
    "  // advance blocktile\n",
    "  A += BK;     // move BK columns to right\n",
    "  B += BK * N; // move BK rows down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c225a4e",
   "metadata": {},
   "source": [
    "Again, these are our standard maintanence commmands we have been running for the previous few kernels, that are inside the main outer loop - they just ensure that warps don't rush ahead to corrupt the SMEM caches and that the pointers are moved appropriately for the next iteration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
